import pandas as pd
from prompts import get_system_prompt, PERSONA_PROFILE, TEST_SCENARIOS
from evaluator import ModelEvaluator

MODELS = [
    {
        "name": "Gemma-2-2B-It",
        "path": "./models/Gemma-2-2b-it-Q4_K_M.gguf"
    },
    {
        "name": "Llama-3.2-3B-Instruct",
        "path": "./models/Llama-3.2-3B-Instruct-Q4_K_M.gguf"
    },
    {
        "name": "Qwen2.5-3B-Instruct",
        "path": "./models/Qwen2.5-3B-Instruct-Q4_K_M.gguf"
    },
    {
        "name": "EXAONE-3.5-2.4B-Instruct",
        "path": "./models/EXAONE-3.5-2.4B-Instruct-Q4_K_M.gguf"
    }
]

def main():
    results = []
    system_prompt = get_system_prompt(PERSONA_PROFILE)

    for model in MODELS:

        evaluator = ModelEvaluator(model['path'], model['name'])
        evaluator.load_model()
        
        if not evaluator.llm:
            continue

        # ì‹œë‚˜ë¦¬ì˜¤ë³„ í…ŒìŠ¤íŠ¸
        print(f"â–¶ {model['name']} í…ŒìŠ¤íŠ¸")
        for scenario in TEST_SCENARIOS:
            result = evaluator.generate_and_measure(system_prompt, scenario)
            if result:
                results.append(result)
                print(f"   [Query] {scenario[:30]}...")
                print(f"   [Resp]  {result['response'][:30]}...")
                print(f"   [Perf]  TTFT: {result['ttft_sec']}s | TPS: {result['tps']}")
        
        # ëª¨ë¸ ì–¸ë¡œë“œ (ë‹¤ìŒ ëª¨ë¸ì„ ìœ„í•´ ë©”ëª¨ë¦¬ ë¹„ìš°ê¸°)
        evaluator.unload_model()

    # 4. ê²°ê³¼ ì €ì¥ ë° ì¶œë ¥
    df = pd.DataFrame(results)
    
    # ê°€ë…ì„±ì„ ìœ„í•œ ì»¬ëŸ¼ ì •ë ¬
    df = df[['model', 'ttft_sec', 'tps', 'input', 'response']]
    
    print("\nğŸ“Š [Final Comparison Report]")
    print(df.groupby('model')[['ttft_sec', 'tps']].mean()) # ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥
    
    # CSV ì €ì¥
    df.to_csv("sllm_persona_test_result.csv", index=False, encoding='utf-8-sig')
    print("\nğŸ’¾ Results saved to 'sllm_persona_test_result.csv'")

if __name__ == "__main__":
    main()
import time
import gc
from llama_cpp import Llama

class ModelEvaluator:
    def __init__(self, model_path, model_name, n_gpu_layers=-1):
        self.model_path = model_path
        self.model_name = model_name
        self.n_gpu_layers = n_gpu_layers
        self.llm = None

    def load_model(self):
        print(f"ëª¨ë¸ ë¡œë“œ ì¤‘...: {self.model_name}")
        try:
            self.llm = Llama(
                model_path=self.model_path,
                n_gpu_layers=self.n_gpu_layers, # -1: ëª¨ë“  ë ˆì´ì–´ GPU í• ë‹¹
                n_ctx=4096,                     # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°
                verbose=False
            )       
            print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def unload_model(self):
        if self.llm:
            del self.llm
            self.llm = None
            gc.collect() # ê°€ë¹„ì§€ ì»¬ë ‰í„° ê°•ì œ ì‹¤í–‰
            print(f"ðŸ—‘ï¸ ëª¨ë¸ í•´ì œ ì™„ë£Œ: {self.model_name}\n")

    def generate_and_measure(self, system_prompt, user_input):
        if not self.llm:
            return None

        # 1. ê¸°ë³¸ ì‹œë„: System Roleê³¼ User Roleì„ ë¶„ë¦¬í•´ì„œ ì „ì†¡
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_input}
        ]

        start_time = time.time()
        first_token_time = None
        token_count = 0
        response_text = ""

        try:
            # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ìƒì„± ì‹œë„
            stream = self.llm.create_chat_completion(
                messages=messages,
                max_tokens=256,
                temperature=0.7,
                stream=True
            )
        except ValueError:
            # [ì—ëŸ¬ í•´ê²° í•µì‹¬] System role not supported ì—ëŸ¬ ë°œìƒ ì‹œ
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ìœ ì € í”„ë¡¬í”„íŠ¸ ì•žë‹¨ì— í•©ì³ì„œ(Merge) ìž¬ì‹œë„
            messages = [
                {"role": "user", "content": f"{system_prompt}\n\nAnswer the user's input based on the instructions above.\n\nUser Input: {user_input}"}
            ]
            stream = self.llm.create_chat_completion(
                messages=messages,
                max_tokens=256,
                temperature=0.7,
                stream=True
            )

        for chunk in stream:
            delta = chunk['choices'][0]['delta']
            if 'content' in delta:
                content = delta['content']
                if not first_token_time:
                    first_token_time = time.time() # ì²« í† í° ë„ì°© ì‹œê°„ ê¸°ë¡
                
                response_text += content
                token_count += 1

        end_time = time.time()
        
        # ì§€í‘œ ê³„ì‚°
        total_time = end_time - start_time
        ttft = (first_token_time - start_time) if first_token_time else total_time
        generation_time = end_time - first_token_time if first_token_time else 0
        tps = token_count / generation_time if generation_time > 0 else 0

        return {
            "model": self.model_name,
            "input": user_input,
            "response": response_text.strip(),
            "ttft": round(ttft, 4),
            "tps": round(tps, 2),
            "total_tokens": token_count
        }
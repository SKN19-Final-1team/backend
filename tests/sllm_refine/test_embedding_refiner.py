"""
ì„ë² ë”© ê¸°ë°˜ í…ìŠ¤íŠ¸ ì •ì œ ìë™í™” í…ŒìŠ¤íŠ¸ ë° ëª¨ë¸ ë¹„êµ
"""

import time
import sys
from pathlib import Path
import numpy as np

# í…ŒìŠ¤íŠ¸ ë°ì´í„° import
sys.path.insert(0, str(Path(__file__).parent / "tests"))
from test_data.noisy_utterances import get_test_dataset

from app.llm.sllm_refiner_embed import refine_text_with_embedding

# ë¹„êµí•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
CANDIDATE_MODELS = [
    "jhgan/ko-sroberta-multitask",       # ë¹ ë¦„, ê¸°ë³¸
    "BM-K/KoSimCSE-roberta-multitask",   # ì„±ëŠ¥ ìš°ìˆ˜
    "jhgan/ko-sbert-nli",                # NLI íŠ¹í™”
]


def run_automated_test(model_name: str = "jhgan/ko-sbert-nli", show_details: bool = True):
    """ìë™í™”ëœ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    if show_details:
        print("=" * 70)
        print(f"ì„ë² ë”© ê¸°ë°˜ í…ìŠ¤íŠ¸ ì •ì œ í…ŒìŠ¤íŠ¸ (ëª¨ë¸: {model_name})")
        print("=" * 70)
    
    # ì´ˆê¸°í™”
    if show_details:
        print(f"\n[ì´ˆê¸°í™”] ëª¨ë¸ ë¡œë”© ë° ì„ë² ë”© ìƒì„± ì¤‘... ({model_name})")
    
    start_init = time.time()
    # ì›œì—… (ëª¨ë¸ ë¡œë“œ ë° ìºì‹œ init)
    _ = refine_text_with_embedding("í…ŒìŠ¤íŠ¸", threshold=0.65, model_name=model_name)
    init_time = time.time() - start_init
    
    if show_details:
        print(f"âœ… ì´ˆê¸°í™” ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {init_time:.2f}s)\n")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    test_data = get_test_dataset()
    if show_details:
        print(f"[í…ŒìŠ¤íŠ¸] ì´ {len(test_data)}ê°œ ì¼€ì´ìŠ¤ ì‹¤í–‰\n")
        print("=" * 70)
    
    # í†µê³„
    total_cases = len(test_data)
    passed_cases = 0
    failed_cases = 0
    total_time = 0
    
    results = []
    
    filter_conf = 0.65  # ì„ê³„ê°’ í†µì¼
    
    for i, (original, noisy, expected_keywords) in enumerate(test_data, 1):
        if show_details:
            print(f"\n[{i}/{total_cases}] í…ŒìŠ¤íŠ¸ ì¤‘...")
            print(f"ì›ë³¸:   {original}")
            print(f"ì…ë ¥:   {noisy}")
        
        # í…ìŠ¤íŠ¸ ì •ì œ
        start = time.time()
        # ëª¨ë¸ëª…ì„ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬
        result = refine_text_with_embedding(noisy, threshold=filter_conf, model_name=model_name)
        elapsed = time.time() - start
        total_time += elapsed
        
        refined = result['text']
        keywords = result['keywords']
        
        passed = False
        
        # ê²€ì¦ (í‚¤ì›Œë“œ ë§¤ì¹­)
        if expected_keywords:
            extracted_kw_set = set(kw.lstrip('#') for kw in keywords)
            expected_kw_set = set(expected_keywords)
            
            matched = len(extracted_kw_set & expected_kw_set)
            if matched > 0:
                if show_details:
                    print(f"âœ… PASS (ë§¤ì¹­: {matched}/{len(expected_keywords)})")
                passed = True
            else:
                if show_details:
                    print(f"âŒ FAIL (ë§¤ì¹­: 0/{len(expected_keywords)})")
                passed = False
        else:
            if show_details:
                print(f"âšª SKIP (ê¸°ëŒ€ í‚¤ì›Œë“œ ì—†ìŒ)")
            passed = True
        
        if passed:
            passed_cases += 1
        else:
            failed_cases += 1
            
        if show_details:
            print(f"ì •ì œ:   {refined}")
            print(f"í‚¤ì›Œë“œ: {', '.join(keywords) if keywords else '(ì—†ìŒ)'}")
            print(f"ì‹œê°„:   {elapsed*1000:.1f}ms")
            print("-" * 70)
            
        results.append({
            'original': original,
            'noisy': noisy,
            'refined': refined,
            'keywords': keywords,
            'expected': expected_keywords,
            'passed': passed,
            'time': elapsed
        })
    
    # í†µê³„ ê³„ì‚°
    accuracy = (passed_cases / total_cases) * 100
    avg_time = (total_time / total_cases) * 1000
    
    if show_details:
        print("\n" + "=" * 70)
        print(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ({model_name})")
        print("=" * 70)
        print(f"ì´ ì¼€ì´ìŠ¤:     {total_cases}ê°œ")
        print(f"í†µê³¼:          {passed_cases}ê°œ ({accuracy:.1f}%)")
        print(f"ì‹¤íŒ¨:          {failed_cases}ê°œ ({failed_cases/total_cases*100:.1f}%)")
        print(f"í‰ê·  ì²˜ë¦¬ì‹œê°„: {avg_time:.1f}ms")
        print(f"ì´ ì†Œìš”ì‹œê°„:   {total_time:.2f}s")
        print("=" * 70)
    
    return {
        'model': model_name,
        'accuracy': accuracy,
        'avg_time': avg_time,
        'passed': passed_cases,
        'failed': failed_cases,
        'total_time': total_time,
        'init_time': init_time,
        'results': results
    }


def run_model_comparison():
    """ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ ì‹¤í–‰"""
    print("\n" + "=" * 80)
    print(f"ğŸš€ ì„ë² ë”© ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ({len(CANDIDATE_MODELS)}ê°œ ëª¨ë¸)")
    print("=" * 80)
    print(f"ëŒ€ìƒ ëª¨ë¸: {', '.join(CANDIDATE_MODELS)}")
    
    comparison_results = []
    
    for i, model in enumerate(CANDIDATE_MODELS, 1):
        print(f"\n\n[{i}/{len(CANDIDATE_MODELS)}] ëª¨ë¸ í‰ê°€ ì¤‘: {model}")
        print("-" * 40)
        # ìƒì„¸ ë¡œê·¸ëŠ” ë„ê³  ê²°ê³¼ë§Œ ìˆ˜ì§‘
        result = run_automated_test(model_name=model, show_details=True)
        comparison_results.append(result)
    
    # ìµœì¢… ë¹„êµ í…Œì´ë¸” ì¶œë ¥
    print("\n\n" + "=" * 100)
    print(f"{'Rank':<5} {'Model Name':<40} {'Accuracy':<10} {'Avg Time':<10} {'Init Time':<10}")
    print("-" * 100)
    
    # ì •í™•ë„ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    comparison_results.sort(key=lambda x: x['accuracy'], reverse=True)
    
    for rank, res in enumerate(comparison_results, 1):
        print(f"{rank:<5} {res['model']:<40} {res['accuracy']:.1f}%     {res['avg_time']:.1f}ms    {res['init_time']:.1f}s")
    print("=" * 100)
    
    # ìµœê³  ëª¨ë¸ ì¶”ì²œ
    best_model = comparison_results[0]
    print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model['model']} (ì •í™•ë„: {best_model['accuracy']:.1f}%)")
    
    return comparison_results


def run_interactive_test():
    """ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸"""
    print("=" * 70)
    print("ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
    print("=" * 70)
    
    # ëª¨ë¸ ì„ íƒ
    print("ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:")
    for i, model in enumerate(CANDIDATE_MODELS, 1):
        print(f"{i}. {model}")
    
    try:
        choice = int(input("\nì„ íƒ (1~3, ê¸°ë³¸ 1): ") or 1)
        model_name = CANDIDATE_MODELS[choice-1]
    except:
        model_name = CANDIDATE_MODELS[0]
    
    print(f"\nì„ íƒëœ ëª¨ë¸: {model_name}")
    print("ì´ˆê¸°í™” ì¤‘...")
    _ = refine_text_with_embedding("í…ŒìŠ¤íŠ¸", threshold=0.65, model_name=model_name)
    print("âœ… ì´ˆê¸°í™” ì™„ë£Œ!\n")
    
    while True:
        try:
            user_input = input("\nì…ë ¥ (ì¢…ë£Œ: q): ").strip()
            
            if user_input.lower() in ['q', 'quit', 'ì¢…ë£Œ']:
                break
            
            if not user_input:
                continue
            
            start = time.time()
            result = refine_text_with_embedding(user_input, threshold=0.65, model_name=model_name)
            elapsed = time.time() - start
            
            print(f"\n{'='*70}")
            print(f"ì›ë³¸:   {user_input}")
            print(f"ì •ì œ:   {result['text']}")
            print(f"í‚¤ì›Œë“œ: {', '.join(result['keywords']) if result['keywords'] else '(ì—†ìŒ)'}")
            print(f"ì‹œê°„:   {elapsed*1000:.1f}ms")
            print(f"{'='*70}")
        
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"\nì˜¤ë¥˜ ë°œìƒ: {e}")


def main():
    if len(sys.argv) > 1:
        if sys.argv[1] == '--compare':
            run_model_comparison()
        elif sys.argv[1] == '--auto':
            run_automated_test()
        else:
            run_interactive_test()
    else:
        # ì¸ì ì—†ìœ¼ë©´ ë©”ë‰´ í‘œì‹œ
        print("1. ëª¨ë¸ ë¹„êµ ì‹¤í–‰ (--compare)")
        print("2. ë‹¨ì¼ ëª¨ë¸ ìë™ í…ŒìŠ¤íŠ¸ (--auto)")
        print("3. ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ (ê¸°ë³¸)")
        
        choice = input("\nì„ íƒ (1~3): ").strip()
        
        if choice == '1':
            run_model_comparison()
        elif choice == '2':
            run_automated_test()
        else:
            run_interactive_test()


if __name__ == "__main__":
    main()

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1a9aca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PERSONA_PROFILE' from 'config' (c:\\Projects\\backend\\tests\\config.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_system_prompt, PERSONA_PROFILE, TEST_SCENARIOS\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mevaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelEvaluator\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'PERSONA_PROFILE' from 'config' (c:\\Projects\\backend\\tests\\config.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from config import get_system_prompt, PERSONA_PROFILE, TEST_SCENARIOS\n",
    "from evaluator import ModelEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c18497b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    {\n",
    "        \"name\": \"Gemma-2-2B-It\",\n",
    "        \"path\": \"./models/Gemma-2-2b-it-Q4_K_M.gguf\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Llama-3.2-3B-Instruct\",\n",
    "        \"path\": \"./models/Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Qwen2.5-3B-Instruct\",\n",
    "        \"path\": \"./models/Qwen2.5-3B-Instruct-Q4_K_M.gguf\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"EXAONE-3.5-2.4B-Instruct\",\n",
    "        \"path\": \"./models/EXAONE-3.5-2.4B-Instruct-Q4_K_M.gguf\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdcc2ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    {\n",
    "        \"name\": \"kanana-nano-2.1b-instruct.Q4_K_M\",\n",
    "        \"path\": \"./models/kanana-nano-2.1b-instruct.Q4_K_M.gguf\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Llama-3-Kor-BCCard-Finance-8B.Q4_K_M\",\n",
    "        \"path\": \"./models/Llama-3-Kor-BCCard-Finance-8B.Q4_K_M.gguf\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "767b3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from config import SOURCE_TRANSCRIPT, ANALYSIS_PROMPT, get_actor_system_prompt, TEST_SCENARIOS\n",
    "from evaluator import ModelEvaluator\n",
    "\n",
    "def main():\n",
    "    final_report = []\n",
    "\n",
    "    print(\"üé≠ [sLLM Qualitative Persona Test Started]\")\n",
    "    print(\"   Target: Analyze raw transcript -> Generate persona -> Act\\n\")\n",
    "\n",
    "    for model in MODELS:\n",
    "        model_name = model['name']\n",
    "        print(f\"==================================================\")\n",
    "        print(f\"ü§ñ Testing Model: {model_name}\")\n",
    "        print(f\"==================================================\")\n",
    "\n",
    "        evaluator = ModelEvaluator(model['path'], model_name)\n",
    "        evaluator.load_model()\n",
    "        \n",
    "        if not evaluator.llm: continue\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # Step 1. ÌéòÎ•¥ÏÜåÎÇò Î∂ÑÏÑù (Extraction)\n",
    "        # ---------------------------------------------------------\n",
    "        # Î∂ÑÏÑùÏùÑ ÏúÑÌï¥ ÎåÄÎ≥∏ + ÏßÄÏãúÏÇ¨Ìï≠ÏùÑ Ìï©Ï≥êÏÑú ÏûÖÎ†•\n",
    "        full_analysis_input = f\"{SOURCE_TRANSCRIPT}\\n\\n{ANALYSIS_PROMPT}\"\n",
    "        \n",
    "        # System Prompt ÏóÜÏù¥ ÏàúÏàò Î∂ÑÏÑù Îä•Î†• ÌÖåÏä§Ìä∏ (User PromptÎßå ÏÇ¨Ïö©)\n",
    "        analysis_result = evaluator.generate_and_measure(\n",
    "            system_prompt=\"ÎãπÏã†ÏùÄ Ïú†Îä•Ìïú Î∂ÑÏÑùÍ∞ÄÏûÖÎãàÎã§.\", \n",
    "            user_input=full_analysis_input\n",
    "        )\n",
    "        \n",
    "        extracted_persona = analysis_result['response']\n",
    "        print(f\"{extracted_persona.replace(chr(10), ' ')}\")\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # Step 2. ÌéòÎ•¥ÏÜåÎÇò Ï£ºÏûÖ Î∞è Ïó∞Í∏∞ (Roleplay)\n",
    "        # ---------------------------------------------------------\n",
    "        \n",
    "        # Î™®Îç∏Ïù¥ Ïä§Ïä§Î°ú Ï∂îÏ∂úÌïú ÌéòÎ•¥ÏÜåÎÇòÎ•º Îã§Ïãú ÏûêÏã†Ïùò ÏãúÏä§ÌÖú ÌîÑÎ°¨ÌîÑÌä∏Î°ú Ï£ºÏûÖ\n",
    "        actor_system_prompt = get_actor_system_prompt(extracted_persona)\n",
    "\n",
    "        for i, scenario in enumerate(TEST_SCENARIOS):\n",
    "            print(f\"Q{i+1}. ÏÉÅÎã¥ÏÇ¨: {scenario}\")\n",
    "            \n",
    "            roleplay_result = evaluator.generate_and_measure(\n",
    "                system_prompt=actor_system_prompt,\n",
    "                user_input=scenario\n",
    "            )\n",
    "            \n",
    "            reply = roleplay_result['response']\n",
    "            print(f\"{model_name}: {reply}\\n\")\n",
    "\n",
    "            # Í≤∞Í≥º Ï†ÄÏû•\n",
    "            final_report.append({\n",
    "                \"model\": model_name,\n",
    "                \"step\": \"Roleplay\",\n",
    "                \"scenario\": scenario,\n",
    "                \"extracted_persona_summary\": extracted_persona[:100], # Ï∞∏Í≥†Ïö© ÏöîÏïΩ\n",
    "                \"response\": reply\n",
    "            })\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨\n",
    "        # ---------------------------------------------------------\n",
    "        evaluator.unload_model()\n",
    "\n",
    "    # Í≤∞Í≥º ÌååÏùº Ï†ÄÏû•\n",
    "    df = pd.DataFrame(final_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58156fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≠ [sLLM Qualitative Persona Test Started]\n",
      "   Target: Analyze raw transcript -> Generate persona -> Act\n",
      "\n",
      "==================================================\n",
      "ü§ñ Testing Model: kanana-nano-2.1b-instruct.Q4_K_M\n",
      "==================================================\n",
      "Î™®Îç∏ Î°úÎìú Ï§ë...: kanana-nano-2.1b-instruct.Q4_K_M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\n",
      "### Í≥†Í∞ù ÌéòÎ•¥ÏÜåÎÇò ÏöîÏïΩ  1. **ÏÑ±Í≤© Î∞è Í∞êÏ†ï ÏÉÅÌÉú**    - **ÏÑ±Í≤©**: Îã§ÌòàÏßàÏ†ÅÏù¥Í≥† Îß§Ïö∞ Í∞êÏ†ïÏ†ÅÏù¥Î©∞, ÏßßÍ≥† Í∞ïÌïú Ïñ¥Ï°∞Î°ú Î∂àÎßåÏùÑ ÌëúÏ∂ú.    - **Í∞êÏ†ï ÏÉÅÌÉú**: ÌòÑÏû¨ Îß§Ïö∞ Î∂àÏïàÌïòÍ≥† ÌôîÍ∞Ä ÎÇò ÏûàÏúºÎ©∞, ÏÉÅÌô©Ïóê ÎåÄÌïú Ïù¥Ìï¥ Î∂ÄÏ°±ÏúºÎ°ú Ïù∏Ìï¥ Í≥ºÎèÑÌïú Î∞òÏùëÏùÑ Î≥¥ÏûÑ.  2. **ÎßêÌà¨ ÌäπÏßï**    - **ÎßêÌà¨**: Îπ†Î•¥Í≥† ÎÅäÏñ¥ÏßÄÎäî ÎßêÌà¨. Î∞òÎßêÏùÑ ÏÑûÏñ¥ ÏÇ¨Ïö©ÌïòÎ©∞, Í∞êÌÉÑÏÇ¨Î•º ÎÇ®Î∞úÌïòÏó¨ Í∞ïÌïú Í∞êÏ†ïÏùÑ ÎìúÎü¨ÎÉÑ.    - **ÌäπÏßï**: \"ÏïÑÎãà, Ï£ÑÏÜ°ÌïòÎã§Îäî Îßê ÎßêÍ≥†Ïöî.\", \"ÎÇ¥Í∞Ä ÎèàÏùÑ Ïôú ÎØ∏Î¶¨ ÎÉàÎäîÎç∞Ïöî?\"ÏôÄ Í∞ôÏùÄ Í∞ïÌïú Ïñ¥Ï°∞Î°ú ÎßêÌï®.  3. **ÌïµÏã¨ Î∂àÎßå ÏÇ¨Ìï≠**    - Í≥†Í∞ùÏùÄ ÏÑ†Í≤∞Ï†úÎ•º ÌñàÏùåÏóêÎèÑ Î∂àÍµ¨ÌïòÍ≥† Ï≤≠Íµ¨ Í∏àÏï°Ïù¥ Í∑∏ÎåÄÎ°ú ÎÇòÏò§Îäî Í≤ÉÏóê ÎåÄÌï¥ ÌÅ¨Í≤å Î∂àÎßåÏùÑ Í∞ÄÏßÄÍ≥† ÏûàÏùå.    - Ïù¥Îäî Í≥†Í∞ùÏù¥ ÏÑ†Í≤∞Ï†ú Í∏àÏï°Ïù¥ Ï≤≠Íµ¨ÏÑúÏóê Î∞òÏòÅÎêòÏßÄ ÏïäÏïòÎã§Í≥† ÏûòÎ™ª Ïù¥Ìï¥ÌïòÍ≥† ÏûàÍ∏∞ ÎïåÎ¨∏Ïù¥Î©∞, ÏÑ†Í≤∞Ï†ú Í∏àÏï°Ïù¥ ÏûêÎèôÏúºÎ°ú ÏÉÅÍ≥Ñ\n",
      "Q1. ÏÉÅÎã¥ÏÇ¨: Í≥†Í∞ùÎãò, ÎßéÏù¥ ÎãµÎãµÌïòÏÖ®Ï£†? Ï†úÍ∞Ä Îã§Ïãú ÌïúÎ≤à Ï≤úÏ≤úÌûà ÏÑ§Î™ÖÌï¥ ÎìúÎ†§ÎèÑ Îê†ÍπåÏöî?\n",
      "kanana-nano-2.1b-instruct.Q4_K_M: ÏïÑÎãà, Ï£ÑÏÜ°ÌïòÎã§Îäî Îßê ÎßêÍ≥†Ïöî! \n",
      "ÎÇ¥Í∞Ä ÎèàÏùÑ Ïôú ÎØ∏Î¶¨ ÎÉàÎäîÎç∞, Ïù¥Í≤å Í∑∏ÎåÄÎ°ú Ï≤≠Íµ¨ ÎêêÏúºÎ©¥ Ï¢ãÍ≤†Îã§Îäî Í±∞Ï£†? \n",
      "Ïù¥Ìï¥Í∞Ä Ïïà ÎêòÎÑ§Ïöî. \n",
      "ÏÑ†Í≤∞Ï†ú Ìïú Í∏àÏï°Ïù¥ ÏûêÎèôÏúºÎ°ú ÏÉÅÍ≥Ñ ÎèºÏïº ÌïòÎäî Í±∞ ÏïÑÎãåÍ∞ÄÏöî? \n",
      "Ïù¥Ìï¥Í∞Ä Ïïà ÎêòÎãàÍπå Îçî ÏÑ§Î™ÖÌï¥ Ï£ºÏÑ∏Ïöî!\n",
      "\n",
      "Q2. ÏÉÅÎã¥ÏÇ¨: Ï†ÑÏÇ∞ÏÉÅÏúºÎ°úÎäî Ï†ïÏÉÅ Ï≤òÎ¶¨ÎêòÏóàÏúºÎãà Í±±Ï†ïÌïòÏßÄ ÏïäÏúºÏÖîÎèÑ Îê©ÎãàÎã§.\n",
      "kanana-nano-2.1b-instruct.Q4_K_M: ÏïÑÎãà, Ï†ÑÏÇ∞ÏÉÅÏúºÎ°ú Ï†ïÏÉÅ Ï≤òÎ¶¨ÎêêÎã§Í≥†Ïöî? Í∑∏Îüº Ïôú ÎÇ¥ Í≥ÑÏ¢åÏóêÏÑú ÎèàÏù¥ Í∑∏ÎåÄÎ°ú ÎÇ®ÏïÑ ÏûàÎäî Í±∞ÏòàÏöî? ÎÇ¥Í∞Ä ÎèàÏùÑ ÎØ∏Î¶¨ ÎÉàÎäîÎç∞, Í∑∏ ÎèàÏù¥ ÏÉÅÍ≥ÑÎêòÏñ¥Ïïº ÌïòÎäî Í±∞ ÏïÑÎãåÍ∞ÄÏöî? ÎÇ¥ ÎèàÏù¥ Ïôú Ïù¥Î†áÍ≤å ÎÇ®ÏïÑ ÏûàÎäîÏßÄ ÏÑ§Î™Ö Ï¢Ä Ìï¥Î≥¥ÏÑ∏Ïöî!\n",
      "\n",
      "Q3. ÏÉÅÎã¥ÏÇ¨: Í∑∏Îüº Í≥†Í∞ùÎãò, Ïù¥Î≤à Îã¨ Í≤∞Ï†ú ÏòàÏ†ï Í∏àÏï° Î¨∏ÏûêÎ•º Îã§Ïãú Î≥¥ÎÇ¥ÎìúÎ¶¥ÍπåÏöî?\n",
      "kanana-nano-2.1b-instruct.Q4_K_M: ÏïÑÎãà, Í∑∏Îü∞ Îßê ÎßêÍ≥†Ïöî! ÎÇ¥ ÎèàÏùÑ Ïôú ÎØ∏Î¶¨ ÎÉàÎäîÎç∞, Ïù¥Í±∏Î°ú Ìï¥Í≤∞Ïù¥ Ïïà ÎêòÏûñÏïÑ! \n",
      "ÎÇ¥Í∞Ä Î™á Î≤àÏùÑ ÌôïÏù∏ÌñàÎäîÎç∞, Ï≤≠Íµ¨ Í∏àÏï°Ïù¥ Í∑∏ÎåÄÎ°ú ÎÇòÏôÄ ÏûàÎäî Í±∞Ïïº! \n",
      "Ïù¥Ìï¥Í∞Ä Ïïà ÎêòÎãàÍπå ÎÑàÎ¨¥ ÌôîÍ∞Ä ÎÇò. \n",
      "ÎÇ¥Í∞Ä ÎèàÏùÑ ÎØ∏Î¶¨ ÎÇ∏ Í≤å ÎßûÎã§Î©¥, Í∑∏ Í∏àÏï°Ïù¥ Î∞òÎìúÏãú Î∞òÏòÅÎêòÏñ¥ÏïºÏßÄ! \n",
      "Ïù¥Îü∞ ÏãùÏúºÎ°ú ÌïòÎ©¥ Ïïà ÎêòÏßÄ, Ïù¥Í±¥ ÎÑàÎ¨¥ Î∂àÍ≥µÌèâÌïú Í±∞ÏûñÏïÑ!\n",
      "\n",
      "üóëÔ∏è Î™®Îç∏ Ìï¥Ï†ú ÏôÑÎ£å: kanana-nano-2.1b-instruct.Q4_K_M\n",
      "\n",
      "==================================================\n",
      "ü§ñ Testing Model: Llama-3-Kor-BCCard-Finance-8B.Q4_K_M\n",
      "==================================================\n",
      "Î™®Îç∏ Î°úÎìú Ï§ë...: Llama-3-Kor-BCCard-Finance-8B.Q4_K_M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\n",
      "**Í≥†Í∞ù**Îäî Îß§Ïö∞ Î∂àÏïàÏ†ïÌïú Ïã¨Î¶¨ ÏÉÅÌÉúÏóêÏÑú Ïù¥ÏùòÎ•º Ï†úÍ∏∞ÌñàÏäµÎãàÎã§. Ï£ºÎ°ú Í∞êÏ†ïÏ†ÅÏù∏ ÏöîÏÜåÍ∞Ä_DOM- Domain_ÏóêÏÑúÏùò Í∂åÎ¶¨ÏôÄ Ïù¥Ïö© Ïã§Ï†Å Í¥ÄÎ†® ÏÑ†Í≤∞Ï†ú Ï≤òÎ¶¨Ïóê ÎåÄÌïú Ïù¥Ìï¥ Î∂ÄÏ°±ÏúºÎ°ú Î≥¥ÏûÖÎãàÎã§. 'Ïôú'ÏôÄ Í∞ôÏùÄ ÏßàÎ¨∏ÏóêÏÑú Í∞êÏ†ïÏ†Å Í∑ºÍ±∞Í∞Ä Í∞ïÌïòÍ≤å ÎìúÎü¨ÎÇ©ÎãàÎã§.   - **ÎßêÌà¨ ÌäπÏßï:**    - Îßê ÎÅäÍ∏∞   - Î∞òÎßê ÏÑûÍ∏∞   - ÌäπÏ†ï Í∞êÌÉÑÏÇ¨ ÏÇ¨Ïö©   - ÏùòÎ¨∏Î¨∏ Ï£ºÎ°ú ÏßàÎ¨∏ Íµ¨Ï°∞ - **ÌïµÏã¨ Î∂àÎßå ÏÇ¨Ìï≠:**    - ÏÑ†Ï†úÎπÑÏö©Ïù¥ Ï≤≠Íµ¨ÏÑúÏóê Î∞òÏòÅÎêòÏßÄ ÏïäÎäî Ïù¥Ïú†   - Ïù¥Ïö© Ïã§Ï†ÅÏóê Îî∞Îùº ÏÑ†Ï†úÎπÑÏö©Ïù¥ Ï†ÅÏö©ÎêòÎäî Ï†ïÏ±Ö   - Í≤∞Ï†úÏùº Í∏∞Ï§ÄÏúºÎ°ú Ï∞®Í∞ê Ï≤òÎ¶¨ÎêòÎäî ÏÑ†Ï†úÎπÑÏö©       6Ïõî 3Ïùº Í≤∞Ï†úÏùº Í∏∞Ï§ÄÏúºÎ°ú Ïπ¥Îìú Ïù¥Ïö© Í∏∞Í∞ÑÏùÄ 5Ïõî 13Ïùº Î∂ÄÌÑ∞ 6Ïõî 12ÏùºÍπåÏßÄ ÏûÖÎãàÎã§. Í≤∞Ï†úÏùº Í∏∞Ï§Ä Ï†ÑÏõî 1Ïùº Î∂ÄÌÑ∞ Ï†ÑÏõî ÎßêÏùº ÍπåÏßÄ Ïπ¥Îìú Ïù¥Ïö© Ïã§Ï†ÅÏùÑ Í∏∞Ï§ÄÏúºÎ°ú Ïπ¥Îìú ÏÑúÎπÑÏä§Í∞Ä Ï†úÍ≥µÎê©ÎãàÎã§. ÏÑ†Ï†úÎπÑÏö©Ïù¥ Ïπ¥Îìú Ïù¥Ïö© Ïã§Ï†ÅÏóêÏÑú Ï∞®Í∞êÎêòÏñ¥\n",
      "Q1. ÏÉÅÎã¥ÏÇ¨: Í≥†Í∞ùÎãò, ÎßéÏù¥ ÎãµÎãµÌïòÏÖ®Ï£†? Ï†úÍ∞Ä Îã§Ïãú ÌïúÎ≤à Ï≤úÏ≤úÌûà ÏÑ§Î™ÖÌï¥ ÎìúÎ†§ÎèÑ Îê†ÍπåÏöî?\n",
      "Llama-3-Kor-BCCard-Finance-8B.Q4_K_M: Í≥†Í∞ùÎãòÍªòÏÑúÎäî Í≤∞Ï†úÏùº Í∏∞Ï§Ä Ï†ÑÏõî 1ÏùºÎ∂ÄÌÑ∞ Ï†ÑÏõî ÎßêÏùºÍπåÏßÄÏùò Ïù¥Ïö© Ïã§Ï†ÅÏúºÎ°ú Ïπ¥Îìú ÏÑúÎπÑÏä§Í∞Ä Ï†úÍ≥µÎê©ÎãàÎã§. ÏÑ†Ï†úÎπÑÏö©ÏùÄ Ïπ¥Îìú Ïù¥Ïö© Ïã§Ï†ÅÏóêÏÑú Ï∞®Í∞êÎêòÎ©∞, 6Ïõî 3Ïùº Í≤∞Ï†úÏùº Í∏∞Ï§ÄÏúºÎ°ú 5Ïõî 13ÏùºÎ∂ÄÌÑ∞ 6Ïõî 12ÏùºÍπåÏßÄÏùò Ïù¥Ïö© Ïã§Ï†ÅÏù¥ ÌôïÏù∏Îê©ÎãàÎã§. Ï∂îÍ∞ÄÏ†ÅÏúºÎ°ú, ÏÑ†Ï†úÎπÑÏö©Ïù¥ Ï≤≠Íµ¨ÏÑúÏóê Î∞òÏòÅÎêòÏßÄ ÏïäÎäî Ïù¥Ïú†Îäî Í∞Å Î∞úÍ∏âÏùÄÌñâÏùò ÏòÅÏóÖÏùº Í∏∞Ï§ÄÏúºÎ°ú Ï≤òÎ¶¨ÎêòÍ∏∞ ÎïåÎ¨∏ÏûÖÎãàÎã§. 5Ïõî 18Ïùº Í≤∞Ï†úÏùº Í∏∞Ï§Ä 4Ïõî 3ÏùºÎ∂ÄÌÑ∞ 4Ïõî 30ÏùºÍπåÏßÄÏùò Ïù¥Ïö© Ïã§Ï†ÅÏù¥ 6Ïõî 15ÏùºÏóê Î∞òÏòÅÏù¥ ÎêòÏóàÎã§Î©¥, Í≤∞Ï†úÏùº 7Ïõî 3ÏùºÏóêÎäî 5Ïõî 3ÏùºÎ∂ÄÌÑ∞ 5Ïõî 31ÏùºÍπåÏßÄÏùò Ïù¥Ïö© Ïã§Ï†ÅÏù¥ Ï∞®Í∞êÎêòÏñ¥Ïïº Ìï©ÎãàÎã§. Í≥†Í∞ùÎãòÍªòÏÑúÎäî Ïù¥Ïö© Ïã§Ï†ÅÏóê Îî∞Îùº ÏÑ†Ï†úÎπÑÏö©Ïù¥ Ï†ÅÏö©ÎêòÎäî Ï†ïÏ±ÖÏùÑ Ï∂©Î∂ÑÌûà Ïù¥Ìï¥ÌïòÍ≥† Ïù¥Ïö©ÌïòÏãúÍ∏∞ Î∞îÎûçÎãàÎã§.\n",
      "\n",
      "Q2. ÏÉÅÎã¥ÏÇ¨: Ï†ÑÏÇ∞ÏÉÅÏúºÎ°úÎäî Ï†ïÏÉÅ Ï≤òÎ¶¨ÎêòÏóàÏúºÎãà Í±±Ï†ïÌïòÏßÄ ÏïäÏúºÏÖîÎèÑ Îê©ÎãàÎã§.\n",
      "Llama-3-Kor-BCCard-Finance-8B.Q4_K_M: Í≥†Í∞ùÎãò, Ï†ÑÏÇ∞ÏÉÅ Ï†ïÏÉÅ Ï≤òÎ¶¨Í∞Ä ÎêòÎ©¥ Ïù¥ÌõÑ Ï≤òÎ¶¨Í∞Ä Ïù¥Î§ÑÏßÄÎäîÎç∞ Î¨∏Ï†úÍ∞Ä ÏóÜÎã§Í≥† ÏïàÎÇ¥ÎìúÎ¶ΩÎãàÎã§. Ï∂îÍ∞ÄÏ†ÅÏù∏ ÌîºÌï¥Í∞Ä ÏóÜÎäîÏßÄ ÌôïÏù∏ÌïòÍ∏∞ÏóêÎäî ÎãπÏÇ¨ÏóêÎäî ÌååÏïÖÏù¥ ÎêòÏßÄ ÏïäÍ∏∞ ÎïåÎ¨∏Ïóê ÎãπÏÇ¨ÏóêÏÑúÎäî Í∑∏ Î∂ÄÎ∂ÑÏóê ÎåÄÌï¥ ÏïàÎÇ¥ ÎìúÎ¶¥ ÏàòÎäî ÏóÜÏùåÏùÑ ÏñëÌï¥ Î∂ÄÌÉÅÎìúÎ¶ΩÎãàÎã§. Ìñ•ÌõÑ Ïú†ÏÇ¨ ÏÉÅÌô© Î∞úÏÉù Ïãú Í≥†Í∞ùÎãòÍ≥º Ïù¥Ïö© ÏÉÅÌíàÏóê ÎåÄÌïú Ïö∞Î¶¨Ïπ¥Îìú Domino ÏãúÏä§ÌÖúÏúºÎ°ú Ï†ïÏÉÅ Ï≤òÎ¶¨Ïù¥Î©¥ ÏãúÏä§ÌÖú Í∏∞Ï§ÄÏúºÎ°ú Í≤∞Ï†úÏùº Í∏∞Ï§Ä Ï†ÑÏõî 14ÏùºÎ∂ÄÌÑ∞ ÎßêÏùºÍπåÏßÄ Íµ≠ÎÇ¥Ïô∏ Í∞ÄÎßπÏ†ê Ïù¥Ïö©Í∏àÏï°ÏùÑ Í∏∞Ï§ÄÏúºÎ°ú ÏÑ†Í≤∞Ï†úÍ∞Ä Ïù¥Î£®Ïñ¥ÏßëÎãàÎã§. Í≥†Í∞ùÎãòÏùò Ïù¥Ïö© ÌïúÎèÑ, Í≤∞Ï†úÏùº Îì±Ïóê Îî∞Îùº Îã§ÏàòÏùò Í≥†Í∞ùÎãòÎì§ÏóêÍ≤å ÎèôÏùºÌïú ÎÇ¥Ïö© ÏïàÎÇ¥ ÎìúÎ¶¨Îäî Ï†ê Ï∞∏Í≥† Î∂ÄÌÉÅÎìúÎ¶ΩÎãàÎã§. Í∏∞ÌÉÄ Î¨∏ÏùòÎäî ÎåÄÌëú Í≥†Í∞ùÏÑºÌÑ∞ 1588-4000 Î≤àÏúºÎ°ú Î¨∏Ïùò Î∂ÄÌÉÅÎìúÎ¶ΩÎãàÎã§.Í∞êÏÇ¨Ìï©ÎãàÎã§.\n",
      "\n",
      "Q3. ÏÉÅÎã¥ÏÇ¨: Í∑∏Îüº Í≥†Í∞ùÎãò, Ïù¥Î≤à Îã¨ Í≤∞Ï†ú ÏòàÏ†ï Í∏àÏï° Î¨∏ÏûêÎ•º Îã§Ïãú Î≥¥ÎÇ¥ÎìúÎ¶¥ÍπåÏöî?\n",
      "Llama-3-Kor-BCCard-Finance-8B.Q4_K_M: Í≥†Í∞ùÎãò, Ïù¥Î≤à Îã¨ Í≤∞Ï†ú ÏòàÏ†ï Í∏àÏï°ÏùÄ 50ÎßåÏõêÏúºÎ°ú ÌôïÏù∏Îê©ÎãàÎã§. Îã§Ïãú ÌôïÏù∏Ïù¥ ÌïÑÏöîÌïòÏãúÎ©¥ Í≥†Í∞ùÏÑºÌÑ∞ÏóêÏÑú ÏïàÎÇ¥Í∞Ä Îê©ÎãàÎã§. Í∏∞ÌÉÄ Î¨∏ÏùòÎäî 1588-4000Î≤àÏúºÎ°ú Ïó∞ÎùΩÏ£ºÏÑ∏Ïöî.\n",
      "\n",
      "üóëÔ∏è Î™®Îç∏ Ìï¥Ï†ú ÏôÑÎ£å: Llama-3-Kor-BCCard-Finance-8B.Q4_K_M\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af02a4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITERATIONS = 5  # Í∞Å ÏßàÎ¨∏Îãπ 5Î≤à Î∞òÎ≥µ\n",
    "SKIP_COLD = True    # Ï≤´ Î≤àÏß∏ Ïã§Ìñâ Í≤∞Í≥ºÎäî ÌÜµÍ≥ÑÏóêÏÑú Ï†úÏô∏\n",
    "\n",
    "def main():\n",
    "    results = []\n",
    "    system_prompt = get_system_prompt(PERSONA_PROFILE)\n",
    "    \n",
    "    for model in MODELS:\n",
    "        evaluator = ModelEvaluator(model['path'], model['name'])\n",
    "        evaluator.load_model()\n",
    "\n",
    "        print(f\"‚ñ∂ {model['name']} ÌÖåÏä§Ìä∏\")\n",
    "\n",
    "        # ÏãúÎÇòÎ¶¨Ïò§Î≥Ñ Î∞òÎ≥µ ÌÖåÏä§Ìä∏\n",
    "        for scenario_idx, scenario in enumerate(TEST_SCENARIOS):\n",
    "            for i in range(NUM_ITERATIONS):\n",
    "                result = evaluator.generate_and_measure(system_prompt, scenario)\n",
    "                \n",
    "                if result:\n",
    "                    # ÏΩúÎìúÏä§ÌÉÄÌä∏ Ï†úÏô∏ Î°úÏßÅ\n",
    "                    if SKIP_COLD and i == 0:\n",
    "                        continue \n",
    "                    \n",
    "                    # Í≤∞Í≥ºÏóê ÌöåÏ∞® Ï†ïÎ≥¥ Ï∂îÍ∞Ä\n",
    "                    result['iteration'] = i + 1\n",
    "                    result['scenario_id'] = scenario_idx + 1\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    print(f\"      [Iter {i+1}] TTFT: {result['ttft']}s | TPS: {result['tps']}\")\n",
    "\n",
    "        # Î™®Îç∏ Ïñ∏Î°úÎìú\n",
    "        evaluator.unload_model()\n",
    "\n",
    "    # Í≤∞Í≥º Ï†ÄÏû• Î∞è Î∂ÑÏÑù\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n[ÌÖåÏä§Ìä∏ Í≤∞Í≥º]\")\n",
    "    # Î™®Îç∏Î≥Ñ Ï†ÑÏ≤¥ ÌèâÍ∑†\n",
    "    summary = df.groupby('model')[['ttft', 'tps']].agg(['mean', 'std'])\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0354bba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 39 key-value pairs and 288 tensors from ./models/Gemma-2-2b-it-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Gemma 2 2b It\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = it\n",
      "llama_model_loader: - kv   4:                           general.basename str              = gemma-2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 2B\n",
      "llama_model_loader: - kv   6:                            general.license str              = gemma\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,2]       = [\"conversational\", \"text-generation\"]\n",
      "llama_model_loader: - kv   8:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   9:                    gemma2.embedding_length u32              = 2304\n",
      "llama_model_loader: - kv  10:                         gemma2.block_count u32              = 26\n",
      "llama_model_loader: - kv  11:                 gemma2.feed_forward_length u32              = 9216\n",
      "llama_model_loader: - kv  12:                gemma2.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv  13:             gemma2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  14:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  15:                gemma2.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  16:              gemma2.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  18:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  19:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  20:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î™®Îç∏ Î°úÎìú Ï§ë...: Gemma-2-2B-It\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  33:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  34:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  35:                      quantize.imatrix.file str              = /models_out/gemma-2-2b-it-GGUF/gemma-...\n",
      "llama_model_loader: - kv  36:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  37:             quantize.imatrix.entries_count i32              = 182\n",
      "llama_model_loader: - kv  38:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  105 tensors\n",
      "llama_model_loader: - type q4_K:  156 tensors\n",
      "llama_model_loader: - type q6_K:   27 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 1.59 GiB (5.21 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      1 '<eos>' is not marked as EOG\n",
      "load: control token:      0 '<pad>' is not marked as EOG\n",
      "load: control token:     47 '<unused40>' is not marked as EOG\n",
      "load: control token:      3 '<unk>' is not marked as EOG\n",
      "load: control token:     55 '<unused48>' is not marked as EOG\n",
      "load: control token:     24 '<unused17>' is not marked as EOG\n",
      "load: control token:      2 '<bos>' is not marked as EOG\n",
      "load: control token:      5 '<2mass>' is not marked as EOG\n",
      "load: control token:     58 '<unused51>' is not marked as EOG\n",
      "load: control token:      4 '<mask>' is not marked as EOG\n",
      "load: control token:     42 '<unused35>' is not marked as EOG\n",
      "load: control token:      9 '<unused2>' is not marked as EOG\n",
      "load: control token:      6 '[@BOS@]' is not marked as EOG\n",
      "load: control token:     44 '<unused37>' is not marked as EOG\n",
      "load: control token:     35 '<unused28>' is not marked as EOG\n",
      "load: control token:      7 '<unused0>' is not marked as EOG\n",
      "load: control token:     43 '<unused36>' is not marked as EOG\n",
      "load: control token:     36 '<unused29>' is not marked as EOG\n",
      "load: control token:      8 '<unused1>' is not marked as EOG\n",
      "load: control token:     41 '<unused34>' is not marked as EOG\n",
      "load: control token:     10 '<unused3>' is not marked as EOG\n",
      "load: control token:     40 '<unused33>' is not marked as EOG\n",
      "load: control token:     11 '<unused4>' is not marked as EOG\n",
      "load: control token:     39 '<unused32>' is not marked as EOG\n",
      "load: control token:     12 '<unused5>' is not marked as EOG\n",
      "load: control token:     38 '<unused31>' is not marked as EOG\n",
      "load: control token:     13 '<unused6>' is not marked as EOG\n",
      "load: control token:     37 '<unused30>' is not marked as EOG\n",
      "load: control token:     14 '<unused7>' is not marked as EOG\n",
      "load: control token:     27 '<unused20>' is not marked as EOG\n",
      "load: control token:     15 '<unused8>' is not marked as EOG\n",
      "load: control token:     28 '<unused21>' is not marked as EOG\n",
      "load: control token:     16 '<unused9>' is not marked as EOG\n",
      "load: control token:     17 '<unused10>' is not marked as EOG\n",
      "load: control token:     18 '<unused11>' is not marked as EOG\n",
      "load: control token:     19 '<unused12>' is not marked as EOG\n",
      "load: control token:     20 '<unused13>' is not marked as EOG\n",
      "load: control token:     21 '<unused14>' is not marked as EOG\n",
      "load: control token:     22 '<unused15>' is not marked as EOG\n",
      "load: control token:     56 '<unused49>' is not marked as EOG\n",
      "load: control token:     23 '<unused16>' is not marked as EOG\n",
      "load: control token:     54 '<unused47>' is not marked as EOG\n",
      "load: control token:     25 '<unused18>' is not marked as EOG\n",
      "load: control token:     53 '<unused46>' is not marked as EOG\n",
      "load: control token:     26 '<unused19>' is not marked as EOG\n",
      "load: control token:     29 '<unused22>' is not marked as EOG\n",
      "load: control token:     30 '<unused23>' is not marked as EOG\n",
      "load: control token:     31 '<unused24>' is not marked as EOG\n",
      "load: control token:     32 '<unused25>' is not marked as EOG\n",
      "load: control token:     46 '<unused39>' is not marked as EOG\n",
      "load: control token:     33 '<unused26>' is not marked as EOG\n",
      "load: control token:     45 '<unused38>' is not marked as EOG\n",
      "load: control token:     34 '<unused27>' is not marked as EOG\n",
      "load: control token:     48 '<unused41>' is not marked as EOG\n",
      "load: control token:     49 '<unused42>' is not marked as EOG\n",
      "load: control token:     50 '<unused43>' is not marked as EOG\n",
      "load: control token:     51 '<unused44>' is not marked as EOG\n",
      "load: control token:     52 '<unused45>' is not marked as EOG\n",
      "load: control token:     57 '<unused50>' is not marked as EOG\n",
      "load: control token:     59 '<unused52>' is not marked as EOG\n",
      "load: control token:     60 '<unused53>' is not marked as EOG\n",
      "load: control token:     61 '<unused54>' is not marked as EOG\n",
      "load: control token:     62 '<unused55>' is not marked as EOG\n",
      "load: control token:     63 '<unused56>' is not marked as EOG\n",
      "load: control token:     64 '<unused57>' is not marked as EOG\n",
      "load: control token:     65 '<unused58>' is not marked as EOG\n",
      "load: control token:     66 '<unused59>' is not marked as EOG\n",
      "load: control token:     67 '<unused60>' is not marked as EOG\n",
      "load: control token:     68 '<unused61>' is not marked as EOG\n",
      "load: control token:     69 '<unused62>' is not marked as EOG\n",
      "load: control token:     70 '<unused63>' is not marked as EOG\n",
      "load: control token:     71 '<unused64>' is not marked as EOG\n",
      "load: control token:     72 '<unused65>' is not marked as EOG\n",
      "load: control token:     73 '<unused66>' is not marked as EOG\n",
      "load: control token:     74 '<unused67>' is not marked as EOG\n",
      "load: control token:     75 '<unused68>' is not marked as EOG\n",
      "load: control token:     76 '<unused69>' is not marked as EOG\n",
      "load: control token:     77 '<unused70>' is not marked as EOG\n",
      "load: control token:     78 '<unused71>' is not marked as EOG\n",
      "load: control token:     79 '<unused72>' is not marked as EOG\n",
      "load: control token:     80 '<unused73>' is not marked as EOG\n",
      "load: control token:     81 '<unused74>' is not marked as EOG\n",
      "load: control token:     82 '<unused75>' is not marked as EOG\n",
      "load: control token:     83 '<unused76>' is not marked as EOG\n",
      "load: control token:     84 '<unused77>' is not marked as EOG\n",
      "load: control token:     85 '<unused78>' is not marked as EOG\n",
      "load: control token:     86 '<unused79>' is not marked as EOG\n",
      "load: control token:     87 '<unused80>' is not marked as EOG\n",
      "load: control token:     88 '<unused81>' is not marked as EOG\n",
      "load: control token:     89 '<unused82>' is not marked as EOG\n",
      "load: control token:     90 '<unused83>' is not marked as EOG\n",
      "load: control token:     91 '<unused84>' is not marked as EOG\n",
      "load: control token:     92 '<unused85>' is not marked as EOG\n",
      "load: control token:     93 '<unused86>' is not marked as EOG\n",
      "load: control token:     94 '<unused87>' is not marked as EOG\n",
      "load: control token:     95 '<unused88>' is not marked as EOG\n",
      "load: control token:     96 '<unused89>' is not marked as EOG\n",
      "load: control token:     97 '<unused90>' is not marked as EOG\n",
      "load: control token:     98 '<unused91>' is not marked as EOG\n",
      "load: control token:     99 '<unused92>' is not marked as EOG\n",
      "load: control token:    100 '<unused93>' is not marked as EOG\n",
      "load: control token:    101 '<unused94>' is not marked as EOG\n",
      "load: control token:    102 '<unused95>' is not marked as EOG\n",
      "load: control token:    103 '<unused96>' is not marked as EOG\n",
      "load: control token:    104 '<unused97>' is not marked as EOG\n",
      "load: control token:    105 '<unused98>' is not marked as EOG\n",
      "load: control token:    106 '<start_of_turn>' is not marked as EOG\n",
      "load: control token: 255999 '<unused99>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: printing all EOG tokens:\n",
      "load:   - 1 ('<eos>')\n",
      "load:   - 107 ('<end_of_turn>')\n",
      "load: special tokens cache size = 249\n",
      "load: token to piece cache size = 1.6014 MB\n",
      "print_info: arch             = gemma2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 8192\n",
      "print_info: n_embd           = 2304\n",
      "print_info: n_layer          = 26\n",
      "print_info: n_head           = 8\n",
      "print_info: n_head_kv        = 4\n",
      "print_info: n_rot            = 256\n",
      "print_info: n_swa            = 4096\n",
      "print_info: is_swa_any       = 1\n",
      "print_info: n_embd_head_k    = 256\n",
      "print_info: n_embd_head_v    = 256\n",
      "print_info: n_gqa            = 2\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 6.2e-02\n",
      "print_info: n_ff             = 9216\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 8192\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 2B\n",
      "print_info: model params     = 2.61 B\n",
      "print_info: general.name     = Gemma 2 2b It\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 256000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 2 '<bos>'\n",
      "print_info: EOS token        = 1 '<eos>'\n",
      "print_info: EOT token        = 107 '<end_of_turn>'\n",
      "print_info: UNK token        = 3 '<unk>'\n",
      "print_info: PAD token        = 0 '<pad>'\n",
      "print_info: LF token         = 227 '<0x0A>'\n",
      "print_info: EOG token        = 1 '<eos>'\n",
      "print_info: EOG token        = 107 '<end_of_turn>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 132 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_REPACK model buffer size =   921.38 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  1623.67 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
      "...............\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.98 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n",
      "llama_kv_cache_unified_iswa: creating non-SWA KV cache, size = 4096 cells\n",
      "llama_kv_cache_unified: layer   0: skipped\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: skipped\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: skipped\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: skipped\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: skipped\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: skipped\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: skipped\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: skipped\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: skipped\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: skipped\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: skipped\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: skipped\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: skipped\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   208.00 MiB\n",
      "llama_kv_cache_unified: size =  208.00 MiB (  4096 cells,  13 layers,  1/1 seqs), K (f16):  104.00 MiB, V (f16):  104.00 MiB\n",
      "llama_kv_cache_unified_iswa: creating     SWA KV cache, size = 4096 cells\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: skipped\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: skipped\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: skipped\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: skipped\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: skipped\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: skipped\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: skipped\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: skipped\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: skipped\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: skipped\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: skipped\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: skipped\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: skipped\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   208.00 MiB\n",
      "llama_kv_cache_unified: size =  208.00 MiB (  4096 cells,  13 layers,  1/1 seqs), K (f16):  104.00 MiB, V (f16):  104.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 2304\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   504.50 MiB\n",
      "llama_context: graph nodes  = 1128\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.name': 'Gemma 2 2b It', 'general.architecture': 'gemma2', 'general.type': 'model', 'general.basename': 'gemma-2', 'general.finetune': 'it', 'general.size_label': '2B', 'gemma2.context_length': '8192', 'general.license': 'gemma', 'gemma2.embedding_length': '2304', 'gemma2.block_count': '26', 'gemma2.feed_forward_length': '9216', 'gemma2.attention.head_count': '8', 'gemma2.attention.head_count_kv': '4', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.attention.key_length': '256', 'gemma2.attention.value_length': '256', 'tokenizer.ggml.eos_token_id': '1', 'gemma2.attn_logit_softcapping': '50.000000', 'general.file_type': '15', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.sliding_window': '4096', 'tokenizer.ggml.model': 'llama', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.pre': 'default', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'quantize.imatrix.chunks_count': '128', 'quantize.imatrix.file': '/models_out/gemma-2-2b-it-GGUF/gemma-2-2b-it.imatrix', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'quantize.imatrix.entries_count': '182'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Î™®Îç∏ Î°úÎìú Ïã§Ìå®: 'Llama' object has no attribute 'n_gpu_layers'\n",
      "‚ñ∂ Gemma-2-2B-It ÌÖåÏä§Ìä∏\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1682.40 ms\n",
      "llama_perf_context_print: prompt eval time =    1682.18 ms /   222 tokens (    7.58 ms per token,   131.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5374.02 ms /    98 runs   (   54.84 ms per token,    18.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    7285.89 ms /   320 tokens\n",
      "llama_perf_context_print:    graphs reused =         94\n",
      "Llama.generate: 221 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m scenario_idx, scenario \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(TEST_SCENARIOS):\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_ITERATIONS):\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m         result = \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_and_measure\u001b[49m\u001b[43m(\u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m     20\u001b[39m             \u001b[38;5;66;03m# ÏΩúÎìúÏä§ÌÉÄÌä∏ Ï†úÏô∏ Î°úÏßÅ\u001b[39;00m\n\u001b[32m     21\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m SKIP_COLD \u001b[38;5;129;01mand\u001b[39;00m i == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\SKN19\\backend\\tests\\evaluator.py:70\u001b[39m, in \u001b[36mModelEvaluator.generate_and_measure\u001b[39m\u001b[34m(self, system_prompt, user_input)\u001b[39m\n\u001b[32m     60\u001b[39m     messages = [\n\u001b[32m     61\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msystem_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAnswer the user\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms input based on the instructions above.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mUser Input: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m}\n\u001b[32m     62\u001b[39m     ]\n\u001b[32m     63\u001b[39m     stream = \u001b[38;5;28mself\u001b[39m.llm.create_chat_completion(\n\u001b[32m     64\u001b[39m         messages=messages,\n\u001b[32m     65\u001b[39m         max_tokens=\u001b[32m256\u001b[39m,\n\u001b[32m     66\u001b[39m         temperature=\u001b[32m0.7\u001b[39m,\n\u001b[32m     67\u001b[39m         stream=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     68\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mchoices\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdelta\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bsjun\\anaconda3\\envs\\final_env\\Lib\\site-packages\\llama_cpp\\llama_chat_format.py:321\u001b[39m, in \u001b[36m_convert_text_completion_chunks_to_chat\u001b[39m\u001b[34m(chunks)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_convert_text_completion_chunks_to_chat\u001b[39m(\n\u001b[32m    319\u001b[39m     chunks: Iterator[llama_types.CreateCompletionStreamResponse],\n\u001b[32m    320\u001b[39m ) -> Iterator[llama_types.ChatCompletionChunk]:\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m    337\u001b[39m \u001b[43m                \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bsjun\\anaconda3\\envs\\final_env\\Lib\\site-packages\\llama_cpp\\llama.py:1322\u001b[39m, in \u001b[36mLlama._create_completion\u001b[39m\u001b[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[39m\n\u001b[32m   1320\u001b[39m finish_reason = \u001b[33m\"\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1321\u001b[39m multibyte_fix = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1340\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mllama_cpp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mllama_token_is_eog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1341\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdetokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompletion_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bsjun\\anaconda3\\envs\\final_env\\Lib\\site-packages\\llama_cpp\\llama.py:914\u001b[39m, in \u001b[36mLlama.generate\u001b[39m\u001b[34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[39m\n\u001b[32m    912\u001b[39m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    915\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx < \u001b[38;5;28mself\u001b[39m.n_tokens:\n\u001b[32m    916\u001b[39m         token = \u001b[38;5;28mself\u001b[39m.sample(\n\u001b[32m    917\u001b[39m             top_k=top_k,\n\u001b[32m    918\u001b[39m             top_p=top_p,\n\u001b[32m   (...)\u001b[39m\u001b[32m    932\u001b[39m             idx=sample_idx,\n\u001b[32m    933\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bsjun\\anaconda3\\envs\\final_env\\Lib\\site-packages\\llama_cpp\\llama.py:648\u001b[39m, in \u001b[36mLlama.eval\u001b[39m\u001b[34m(self, tokens)\u001b[39m\n\u001b[32m    644\u001b[39m n_tokens = \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[32m    645\u001b[39m \u001b[38;5;28mself\u001b[39m._batch.set_batch(\n\u001b[32m    646\u001b[39m     batch=batch, n_past=n_past, logits_all=\u001b[38;5;28mself\u001b[39m._logits_all\n\u001b[32m    647\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[32m    650\u001b[39m \u001b[38;5;28mself\u001b[39m.input_ids[n_past : n_past + n_tokens] = batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bsjun\\anaconda3\\envs\\final_env\\Lib\\site-packages\\llama_cpp\\_internals.py:322\u001b[39m, in \u001b[36mLlamaContext.decode\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: LlamaBatch):\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     return_code = \u001b[43mllama_cpp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_code != \u001b[32m0\u001b[39m:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

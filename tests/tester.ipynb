{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1a9aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from config import get_system_prompt, PERSONA_PROFILE, TEST_SCENARIOS\n",
    "from evaluator import ModelEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c18497b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    {\n",
    "        \"name\": \"Gemma-2-2B-It\",\n",
    "        \"path\": \"./models/Gemma-2-2b-it-Q4_K_M.gguf\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Llama-3.2-3B-Instruct\",\n",
    "        \"path\": \"./models/Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Qwen2.5-3B-Instruct\",\n",
    "        \"path\": \"./models/Qwen2.5-3B-Instruct-Q4_K_M.gguf\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"EXAONE-3.5-2.4B-Instruct\",\n",
    "        \"path\": \"./models/EXAONE-3.5-2.4B-Instruct-Q4_K_M.gguf\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "767b3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     results = []\n",
    "#     system_prompt = get_system_prompt(PERSONA_PROFILE)\n",
    "\n",
    "#     for model in MODELS:\n",
    "#         evaluator = ModelEvaluator(model['path'], model['name'])\n",
    "#         evaluator.load_model()\n",
    "        \n",
    "#         if not evaluator.llm:\n",
    "#             continue\n",
    "\n",
    "#         # ì‹œë‚˜ë¦¬ì˜¤ë³„ í…ŒìŠ¤íŠ¸\n",
    "#         print(f\"â–¶ {model['name']} í…ŒìŠ¤íŠ¸\")\n",
    "#         for scenario in TEST_SCENARIOS:\n",
    "#             result = evaluator.generate_and_measure(system_prompt, scenario)\n",
    "#             if result:\n",
    "#                 results.append(result)\n",
    "#                 print(f\"   [Query] {scenario[:30]}...\")\n",
    "#                 print(f\"   [Resp]  {result['response'][:30]}...\")\n",
    "#                 print(f\"   [Perf]  TTFT: {result['ttft']}s | TPS: {result['tps']}\")\n",
    "        \n",
    "#         # ëª¨ë¸ ì–¸ë¡œë“œ (ë‹¤ìŒ ëª¨ë¸ì„ ìœ„í•´ ë©”ëª¨ë¦¬ ë¹„ìš°ê¸°)\n",
    "#         evaluator.unload_model()\n",
    "\n",
    "#     # ê²°ê³¼ ì €ì¥ ë° ì¶œë ¥\n",
    "#     df = pd.DataFrame(results)\n",
    "#     df = df[['model', 'ttft', 'tps', 'input', 'response']]\n",
    "    \n",
    "#     print(\"\\n[í…ŒìŠ¤íŠ¸ ê²°ê³¼]\")\n",
    "#     print(df.groupby('model')[['ttft', 'tps']].mean()) # ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥\n",
    "    \n",
    "#     # CSV ì €ì¥\n",
    "#     # df.to_csv(\"sllm_persona_test_result.csv\", index=False, encoding='utf-8-sig')\n",
    "#     # print(\"\\nResults saved to 'sllm_persona_test_result.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af02a4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITERATIONS = 5  # ê° ì§ˆë¬¸ë‹¹ 5ë²ˆ ë°˜ë³µ\n",
    "SKIP_COLD = True    # ì²« ë²ˆì§¸ ì‹¤í–‰ ê²°ê³¼ëŠ” í†µê³„ì—ì„œ ì œì™¸\n",
    "\n",
    "def main():\n",
    "    results = []\n",
    "    system_prompt = get_system_prompt(PERSONA_PROFILE)\n",
    "    \n",
    "    for model in MODELS:\n",
    "        evaluator = ModelEvaluator(model['path'], model['name'])\n",
    "        evaluator.load_model()\n",
    "\n",
    "        print(f\"â–¶ {model['name']} í…ŒìŠ¤íŠ¸\")\n",
    "\n",
    "        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ë°˜ë³µ í…ŒìŠ¤íŠ¸\n",
    "        for scenario_idx, scenario in enumerate(TEST_SCENARIOS):\n",
    "            for i in range(NUM_ITERATIONS):\n",
    "                result = evaluator.generate_and_measure(system_prompt, scenario)\n",
    "                \n",
    "                if result:\n",
    "                    # ì½œë“œìŠ¤íƒ€íŠ¸ ì œì™¸ ë¡œì§\n",
    "                    if SKIP_COLD and i == 0:\n",
    "                        continue \n",
    "                    \n",
    "                    # ê²°ê³¼ì— íšŒì°¨ ì •ë³´ ì¶”ê°€\n",
    "                    result['iteration'] = i + 1\n",
    "                    result['scenario_id'] = scenario_idx + 1\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    print(f\"      [Iter {i+1}] TTFT: {result['ttft']}s | TPS: {result['tps']}\")\n",
    "\n",
    "        # ëª¨ë¸ ì–¸ë¡œë“œ\n",
    "        evaluator.unload_model()\n",
    "\n",
    "    # ê²°ê³¼ ì €ì¥ ë° ë¶„ì„\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n[í…ŒìŠ¤íŠ¸ ê²°ê³¼]\")\n",
    "    # ëª¨ë¸ë³„ ì „ì²´ í‰ê· \n",
    "    summary = df.groupby('model')[['ttft', 'tps']].agg(['mean', 'std'])\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0354bba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ ë¡œë“œ ì¤‘...: Gemma-2-2B-It\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n",
      "â–¶ Gemma-2-2B-It í…ŒìŠ¤íŠ¸\n",
      "      [Iter 2] TTFT: 0.0583s | TPS: 17.7\n",
      "      [Iter 3] TTFT: 0.0553s | TPS: 17.63\n",
      "      [Iter 4] TTFT: 0.0563s | TPS: 17.69\n",
      "      [Iter 5] TTFT: 0.0568s | TPS: 17.62\n",
      "      [Iter 2] TTFT: 0.0556s | TPS: 17.99\n",
      "      [Iter 3] TTFT: 0.0561s | TPS: 17.86\n",
      "      [Iter 4] TTFT: 0.0557s | TPS: 17.67\n",
      "      [Iter 5] TTFT: 0.0549s | TPS: 17.9\n",
      "      [Iter 2] TTFT: 0.0561s | TPS: 17.89\n",
      "      [Iter 3] TTFT: 0.0552s | TPS: 17.94\n",
      "      [Iter 4] TTFT: 0.0567s | TPS: 17.65\n",
      "      [Iter 5] TTFT: 0.0607s | TPS: 17.16\n",
      "ğŸ—‘ï¸ ëª¨ë¸ í•´ì œ ì™„ë£Œ: Gemma-2-2B-It\n",
      "\n",
      "ëª¨ë¸ ë¡œë“œ ì¤‘...: Llama-3.2-3B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n",
      "â–¶ Llama-3.2-3B-Instruct í…ŒìŠ¤íŠ¸\n",
      "      [Iter 2] TTFT: 0.1285s | TPS: 15.37\n",
      "      [Iter 3] TTFT: 0.066s | TPS: 15.89\n",
      "      [Iter 4] TTFT: 0.0613s | TPS: 15.6\n",
      "      [Iter 5] TTFT: 0.0623s | TPS: 15.84\n",
      "      [Iter 2] TTFT: 0.063s | TPS: 15.5\n",
      "      [Iter 3] TTFT: 0.064s | TPS: 15.84\n",
      "      [Iter 4] TTFT: 0.0639s | TPS: 15.92\n",
      "      [Iter 5] TTFT: 0.0632s | TPS: 15.75\n",
      "      [Iter 2] TTFT: 0.0626s | TPS: 15.92\n",
      "      [Iter 3] TTFT: 0.0625s | TPS: 15.71\n",
      "      [Iter 4] TTFT: 0.0618s | TPS: 15.69\n",
      "      [Iter 5] TTFT: 0.0638s | TPS: 15.93\n",
      "ğŸ—‘ï¸ ëª¨ë¸ í•´ì œ ì™„ë£Œ: Llama-3.2-3B-Instruct\n",
      "\n",
      "ëª¨ë¸ ë¡œë“œ ì¤‘...: Qwen2.5-3B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n",
      "â–¶ Qwen2.5-3B-Instruct í…ŒìŠ¤íŠ¸\n",
      "      [Iter 2] TTFT: 0.0652s | TPS: 14.99\n",
      "      [Iter 3] TTFT: 0.0612s | TPS: 15.12\n",
      "      [Iter 4] TTFT: 0.0626s | TPS: 15.08\n",
      "      [Iter 5] TTFT: 0.0591s | TPS: 16.08\n",
      "      [Iter 2] TTFT: 0.0608s | TPS: 15.37\n",
      "      [Iter 3] TTFT: 0.0608s | TPS: 15.68\n",
      "      [Iter 4] TTFT: 0.0608s | TPS: 15.14\n",
      "      [Iter 5] TTFT: 0.0617s | TPS: 14.9\n",
      "      [Iter 2] TTFT: 0.0621s | TPS: 14.75\n",
      "      [Iter 3] TTFT: 0.0598s | TPS: 15.03\n",
      "      [Iter 4] TTFT: 0.0647s | TPS: 15.17\n",
      "      [Iter 5] TTFT: 0.0605s | TPS: 15.63\n",
      "ğŸ—‘ï¸ ëª¨ë¸ í•´ì œ ì™„ë£Œ: Qwen2.5-3B-Instruct\n",
      "\n",
      "ëª¨ë¸ ë¡œë“œ ì¤‘...: EXAONE-3.5-2.4B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n",
      "â–¶ EXAONE-3.5-2.4B-Instruct í…ŒìŠ¤íŠ¸\n",
      "      [Iter 2] TTFT: 0.0502s | TPS: 20.74\n",
      "      [Iter 3] TTFT: 0.0486s | TPS: 20.71\n",
      "      [Iter 4] TTFT: 0.0504s | TPS: 20.8\n",
      "      [Iter 5] TTFT: 0.05s | TPS: 20.72\n",
      "      [Iter 2] TTFT: 0.0512s | TPS: 19.26\n",
      "      [Iter 3] TTFT: 0.0502s | TPS: 20.25\n",
      "      [Iter 4] TTFT: 0.0493s | TPS: 20.43\n",
      "      [Iter 5] TTFT: 0.0499s | TPS: 19.42\n",
      "      [Iter 2] TTFT: 0.0491s | TPS: 20.04\n",
      "      [Iter 3] TTFT: 0.0524s | TPS: 19.24\n",
      "      [Iter 4] TTFT: 0.0487s | TPS: 20.24\n",
      "      [Iter 5] TTFT: 0.0496s | TPS: 20.04\n",
      "ğŸ—‘ï¸ ëª¨ë¸ í•´ì œ ì™„ë£Œ: EXAONE-3.5-2.4B-Instruct\n",
      "\n",
      "\n",
      "[í…ŒìŠ¤íŠ¸ ê²°ê³¼]\n",
      "                              ttft                  tps          \n",
      "                              mean       std       mean       std\n",
      "model                                                            \n",
      "EXAONE-3.5-2.4B-Instruct  0.049967  0.001068  20.157500  0.578574\n",
      "Gemma-2-2B-It             0.056475  0.001610  17.725000  0.222036\n",
      "Llama-3.2-3B-Instruct     0.068575  0.018911  15.746667  0.181024\n",
      "Qwen2.5-3B-Instruct       0.061608  0.001824  15.245000  0.379605\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

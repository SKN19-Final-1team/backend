{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a261f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers datasets peft accelerate bitsandbytes trl huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05890ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7198ae077bba460582eaf173f2e58fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Hugging Face 로그인\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "435bcfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8320587",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'Qwen/Qwen2.5-3B-Instruct'\n",
    "output_dir = \"./qwen2.5-3b-customer-anlysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eedc3ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac6eb9fa8924ad5b39c33409af7ba5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e7c932c30b41cc9b16331d5c142d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5809ae2e2424fd18f75e3d74ef2e8d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ed4eb05aca4b0a82c25948f58eb67e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83562c7de244c939127f17239aecb58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28887e5562fc41e189ca3b5be5c9604e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e578394be88c498386e13b5d7b591cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7508bbb8866b4c1fadf0b506190775a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ca526e69354853ba473dd05c65c76d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db335046ea734a92b893d7330ae7d77a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e69ffbc4b7486784d3f9c6a8f3e504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4-bit 양자화된 모델 로드를 위한 설정\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',             # 양자화 타입 지정\n",
    "    bnb_4bit_use_double_quant=True,        # 이중 양자화(양자화된 가중치를 한번 더 양자화)\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # 16bit의 부동소수점 데이터 타입 사용\n",
    ")\n",
    "\n",
    "# 모델 및 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 모델 학습 준비\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21055fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # Qwen의 레이어 이름\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40eea80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "상담 스크립트에서 고객의 성향을 분류하세요.\n",
    "전체적인 맥락을 참고하되 판단의 근거는 고객의 발화에 한정합니다.\n",
    "\n",
    "### 분류 규칙\n",
    "1. 제시된 성향 키워드 중 가장 적절한 한가지만 선택\n",
    "2. 오직 키워드만 추출 (예: N3)\n",
    "\n",
    "### 성향 키워드 목록\n",
    "- N1 (실용주의형): 불필요한 말 없이 바로 문의사항을 말함 \n",
    "- N2 (친화적수다형): 사적인 이야기나 본인 상황을 길게 설명함 \n",
    "- N3 (신중/보안 중시형): 신중하고 의심을 보임 \n",
    "- S1 (급한성격형): 빠른 처리를 선호함 \n",
    "- S2 (꼼꼼상세형): 상세한 설명을 요구함 \n",
    "- S3 (이해부족형): 설명을 잘 이해하지 못하여 반복적으로 확인함 \n",
    "- S4 (반복민원형): 과거의 상담 이력을 언급하며 해결되지 않은 불만을 제기함 \n",
    "- S5 (불만형): 분노, 짜증을 드러냄\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbb4c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b762a0ff-1334-4918-b99f-6e58d5f661ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source_id', 'consulting_content', 'personality', 'evidence_sentence'],\n",
       "    num_rows: 4000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef739a5a-e61b-431d-99bf-b08beb9559a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f6e10539a24540ba9e4f04fbd830ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 2:19:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.344000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.068200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.921300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.818200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.798500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.728100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.702000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.717000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.696000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.696500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.634000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.628200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.628000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.618400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.621100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=0.77465220896403, metrics={'train_runtime': 8373.6394, 'train_samples_per_second': 1.433, 'train_steps_per_second': 0.09, 'total_flos': 2.06785461878784e+17, 'train_loss': 0.77465220896403, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.rename_columns({col: col.strip() for col in dataset.column_names})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    texts = []\n",
    "    for script, output in zip(examples['consulting_content'], examples['personality']):\n",
    "        text = (\n",
    "            f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n\"\n",
    "            f\"<|im_start|>user\\n{script}<|im_end|>\\n\"\n",
    "            f\"<|im_start|>assistant\\n{output}<|im_end|>\"\n",
    "        )\n",
    "        texts.append(text)\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        padding=\"max_length\", \n",
    "        return_tensors=None,\n",
    "    )\n",
    "    \n",
    "    tokenized[\"labels\"] = [list(ids) for ids in tokenized[\"input_ids\"]]\n",
    "    return tokenized\n",
    "\n",
    "# 데이터셋 변환 (컬럼 삭제 필수)\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "# 학습 설정\n",
    "# 패딩이 완료되었으므로 가장 기본적인 Collator를 사용하거나 생략 가능합니다.\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2, # 메모리 부족 시 1로 변경\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False # labels를 유지하기 위해 False 설정\n",
    ")\n",
    "\n",
    "# Trainer 실행\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82606508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 완료. 모델이 ./qwen2.5-3b-customer-anlysis에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"학습 완료. 모델이 {output_dir}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7370f2a-1bc1-465b-a4d7-cc33406516b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56fdf72185324103a24b988df66a6f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674423a202314bd886d8028ac4c2d596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22d41e3a0f44be29c099d9eda581a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b65c3782e3437bbe4409cffd8ef3f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b55c1d3a3e4537903d1d9b2a35817b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ansui/qwen2.5-3b-customer-anlysis/commit/6d99ae781c45e5535873bacfaa50a2ff161501ed', commit_message='Upload tokenizer', commit_description='', oid='6d99ae781c45e5535873bacfaa50a2ff161501ed', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ansui/qwen2.5-3b-customer-anlysis', endpoint='https://huggingface.co', repo_type='model', repo_id='ansui/qwen2.5-3b-customer-anlysis'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 로컬에 저장된 모델과 토크나이저 업로드\n",
    "repo_id = 'ansui/qwen2.5-3b-customer-anlysis'\n",
    "\n",
    "trainer.model.push_to_hub(repo_id)\n",
    "tokenizer.push_to_hub(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2d54aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fbd436931c46fd89e6d057d6af8b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류 결과: S1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model_id = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "adapter_model_id = \"ansui/qwen2.5-3b-customer-anlysis\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_model_id)\n",
    "\n",
    "# 베이스 모델 로드\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 베이스 모델에 LoRA 어댑터 입히기\n",
    "model = PeftModel.from_pretrained(base_model, adapter_model_id)\n",
    "model.eval()\n",
    "\n",
    "# 테스트 함수 정의\n",
    "def analyze_customer(script):\n",
    "    prompt = (\n",
    "        f\"<|im_start|>system\\n{SYSTEM_PROMPT}\"\n",
    "        f\"<|im_start|>user\\n{script}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=10, \n",
    "            temperature=0.1,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result.split(\"assistant\\n\")[-1].strip()\n",
    "\n",
    "# 실제 테스트\n",
    "test_script = \"\"\"\n",
    "상담사: 무엇을 도와드릴까요?\n",
    "손님: 아니 카드 결제 문자가 왜 두 번이나 와요? 빨리 확인해서 취소해 줘요!\n",
    "\"\"\"\n",
    "print(f\"분류 결과: {analyze_customer(test_script)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "337a97a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8acdd4d94843c99a365e685d8715fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3db97bd2142414a9b1b4291f86bedc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ffab1366724b569cc7c1cf11f71da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f9e2c623454d7f9b4d6226e73a6bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88dbd9f9925441e3bf4faa46d3482295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555394c37ea84095a114059c5b6c2b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b080ffd946fe4bbaad811c766718d572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ac58d9100c4cf2937d903b42f26cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1ea1616bd343f18e9e92755634d6da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557be00c2cec49cd95e870685685367e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d02a7258f445e2a075b351810b2022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507810b6c8ec45af8ba998ceaaea3f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fbd1051916d4a0fba832d49aa9448b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/120M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging layers...\n",
      "Pushing to Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c997380b8f94a4f8b11abed526b35c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663123a56ddb491a816507fd8f1d174d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7162b0216be24913886657e9fbe206de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925d8e7db62c49839e51a72439292945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074366d219434a4090b1fb5b6fc811e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "업로드 완료\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "base_model_id = \"Qwen/Qwen2.5-3B-Instruct\" # 혹은 사용하신 베이스 모델 ID\n",
    "adapter_id = \"ansui/qwen2.5-3b-customer-anlysis\" # 현재 어댑터 리포지토리\n",
    "new_repo_id = \"ansui/qwen2.5-3b-customer-analysis-merged\" # 새로 만들 리포지토리 이름\n",
    "\n",
    "# 베이스 모델과 토크나이저 로드\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cpu\" # 메모리 확보를 위해 CPU에서 병합 권장\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "\n",
    "# 어댑터 연결\n",
    "model = PeftModel.from_pretrained(base_model, adapter_id)\n",
    "\n",
    "# 모델 병합\n",
    "print(\"Merging layers...\")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# 허깅페이스에 병합된 모델 업로드\n",
    "print(\"Pushing to Hub...\")\n",
    "merged_model.push_to_hub(new_repo_id)\n",
    "tokenizer.push_to_hub(new_repo_id)\n",
    "\n",
    "print(\"업로드 완료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
